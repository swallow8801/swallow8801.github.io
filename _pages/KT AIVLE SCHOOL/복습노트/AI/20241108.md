---
title: "10주차 복습노트"
tags:
    - NLP
date: "2024-11-05"
thumbnail: "/assets/img/thumbnail/nlp.png"
---

# 언어지능 딥러닝
---
10주차는 언어지능 딥러닝에 대해 배웠습니다.

# NLP (Natural Language Processing)
---
## TF-IDF
- Term Frequency-inverse document frequency
- Corpus(문서집합)에서 한 단어가 얼마나 중요한지 수치적으로 나타낸 가중치
- $TF-IDF = TF(t,d) * IDF(t,D)$
- **뉴스 RSS Crawling을 통한 실습**

### 0. Import Library
```python
!pip install feedparser #설치: RSS에서 xml태그별 정보를 추출(예: title,link)
!pip install newspaper3k #설치: 인터넷신문기사분석(아래에 보면 "Article()"을 사용하기 위한 목적)
!pip install konlpy #설치: 한국어형태소분석기(주어진 문장에서 명사만 추출할 목적)
!pip install lxml[html_clean] # 신규추가(2024년 가을)
import feedparser
from newspaper import Article
from konlpy.tag import Okt
from collections import Counter # 명사를 추출한 후에 본문에 몇번이나 그 명사가 나오는지 확인(TF구현용)
```

### 1. RSS(.xml) title/link 추출
```python
urls = ["http://rss.etnews.com/Section901.xml",
        "http://rss.etnews.com/Section902.xml",
        "http://rss.etnews.com/Section903.xml",
        "http://rss.etnews.com/Section904.xml"]
# 아래의 함수는 RSS목록의 list인 urls를 받아서 그 list에 존재하는
# 모든 RSS의 안에 있는 모든 기사들의 title과 link를 추출함
def crawl_rss(urls):
  array_rss = [] # 함수시작하는 시점에 빈 리스트를 만듦. 여기에 모든 기사의 title과 link를 넣을 것
  for index_url in urls: # urls리스트안의 하나씩 xml을 방문(4번 방문: 901,902,903,904)
    print('[Crawl rss]', index_url) # 현재 어떤 xml파일을 방문증인지 표시
    parse_rss = feedparser.parse(index_url) #현재 xml파일을 파싱후 , 결과를 parse_rss에 저장
    for p in parse_rss.entries: # parse_rss에 있는 모든 entries/기사를 검색하면서
      # array_rss의 title 과 link 와 똑같은게 없다면 ( 중복 제거 )
      if not any(article['title'] == p.title and article['link'] == p.link for article in array_rss):
        array_rss.append({'title': p.title, 'link': p.link}) # append 함수로 array_res에 title과 link 붙여넣음
  return array_rss

list_articles = crawl_rss(urls)
print(list_articles)
```

### 2. 모든 기사의 link를 거쳐 본문 title,text 추출
```python
def crawl_article(url, language='ko'):
  print("[Crawl Article]",url) # 현재 text를 추출할 기사의 url 출력
  a=Article(url, language=language) # Article을 사용하여 그 url을 입력하고, 언어옵션 지정
  a.download() # 해당하는 url기사 다운로드
  a.parse() # 해당하는 url기사 분석
  return a.title, a.text # 해당하는 url기사 title과 text 반환

for article in list_articles: # 기존에 만든 list_articles에서 하나하나의 기사를 방문하면서
  _, text = crawl_article(article['link']) # 그 기사의 link를 crawl_article함수에 넣어 본문 추출
  article['text'] = text # 그 추출한 본문의 list_articles에 'text' 속성으로 새로 만들어 저장
print(list_articles[0])
```

### 3. 본문의 Text에서 명사 추출(키워드,빈도수)
```python
def get_keywords(text,nKeywords = 10): # 키워드추출함수 (빈도수고려: TF), 디폴트로 10개
  list_keywords = [] # 비어있는 키워드리스트 <- 추후에 append를 써서 모든 기사에 대한 내용 추가
  spliter = Okt() # konlpy에 의해서 문장을 형태소별로 쪼개는 기능을 위해 spliter 생성
  nouns = spliter.nouns(text) # 입력받은 text를 nouns함수에 넣어서 명사만 추출한 nouns리스트에 넣음
  count = Counter(nouns) # 추출된 명사들의 출현빈도를 저장
  for n, c in count.most_common(nKeywords): # 가장출현빈도 높은 명사부터 순차적으로 10번 연산
    item = {'keyword':n, 'count':c} # 리스트에 저장은 {'keyword','count'} 의 형식으로 함
    list_keywords.append(item) # 위의 포맷으로 list_keywords에 저장
  return list_keywords

for article in list_articles: # 기존에 만든 list_articles에서 하나하나의 기사를 방문하면서
  keywords = get_keywords(article['text']) # 그 기사의 text를 get_keywords함수에 넣어 키워드/빈도 추출
  article['keywords'] = keywords # 그 추출한 키워드와 빈도수를 list_articles에 저장
print(list_articles[0])
```

### 4. 검색어를 입력받아서 그 문서를 출력 (검색엔진 구현)
```
query = input() # 쿼리를 입력받음

# 아래의 함수는 쿼리를 입력받은 후에, 정해진 문서의 keywords의 리스트에서
# 쿼리가 그 keyword 중의 하나로 존재하는 지 검색
def search_articles(query, list_keywords): # 쿼리가 키워드리스트에 있으면 빈도수 출력(없으면 0)
  nWords = 0  # 아래의 if문에 걸리지 않으면 ( 즉, 쿼리가 키워드에 없으면 ) 0을 출력하기 위한 초기값
  for kw in list_keywords: # 키워드 리스트를 하나씩 kw로 검색
    if query == kw['keyword']: # 만약에 쿼리와 동일한 키워드가 존재한다면
      nWords = kw['count'] # 그렇다면, 그 키워드에 해당하는 count가 nWords가 됨
  return nWords # 결과적으로 쿼리의 출현횟수가 출력됨

for article in list_articles: # 기존에 만든 list_articles에서 하나하나의 기사를 방문하면서
  nQuery = search_articles(query,article['keywords']) # search_articles함수로 쿼리의 빈도수 추출
  if nQuery != 0: # 만약에 쿼리의 빈도수가 0이 아니면, 쿼리를 키워드로 가지고 있는 문서이므로, 관련 정보 출력
    print('[TF]',nQuery,'[Title]',article['title'])
    print('[URL]',article['link'])
```

## 단어표현 및 유사도
---
- Word Representation, Word Embedding , Word Vector
- 언어적인 특성을 반영하여 단어를 수치화 하는 방법 -> **Vector**
- 벡터의 크기가 작으면서 단어의 의미를 표현하는 법 -> **분포가설**
    - **카운트 기반 방법** : 문맥 안에서 동시에 등장하는 횟수를 셈
    - **예측 방법** : 신경망 등을 통해 단어를 예측

### 단어 유사도
 - 자카드 유사도 (Jaccard Similarity) : 각각 단어의 집합으로 만든 뒤 두 집합을 통해 유사도 측정
    - 0과 1사이의 값
    - 측정법 : A/B ( 두집합의 교집합 단어 / 합집합 단어 )
- 코사인 유사도 : 두 개의 벡터값에서 코사인 각도를 구함.
      - -1과 1사이의 값


## Word2Vec & Word Embedding
나중에 작성.




# PLM, LLM, RAG
---

## PLM 
- Pretrained Language Model : 대규모 텍스트 데이터로 사전 학습된 언어 모델
- 대량의 언어 데이터를 통해 언어 구조와 의미를 이해함.
    - BERT(Bidirectional Encoder Representations from Transformers)
    - GPT
- Hugging Face & PLM
    - PLM을 쉽게 사용할 수 있도록 도와주는 라이브러리 제공. Token API 사용
    - BERT GPT 등의 모델을 불러와 NLP 작업에 사용할 수 있음.

## LLM
- Large Language Model : 이해하는 데에 그치지 않고 텍스트 생성, 번역 및 요약 등 생성형 AI로 활용되는 언어 모델
- GPT , Llama
- LoRA , QLoRa
- 단점
    - Hallucination : 개체 간의 정보, 사건 등을 혼합하여 그럴듯한 문장 창조
    - 최신 정보 미 반영


## RAG
![RAG](/assets/img/boknote/RAG.PNG)

- Retrieval Augmented Generation : 검색 증강 생성 , LLM의 성능을 문서DB를 검색하여 개선하는 기술
- 문서 검색 : 샘픅을 잘 설명하는 특징 벡터를 통한 검색 ( 유사도 , L1/L2 Distance )
    - Sentence BERT
    - ??

### LangChain
